{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NDMtT52LqjvCw31axQ28MITFWIZOn6yf","timestamp":1745507588411},{"file_id":"12ozIShf_9ZGcAHxDDKNNV4GBhtrxc3SU","timestamp":1745499872965}],"authorship_tag":"ABX9TyO1U1qs6o+DUNCSTpxN30zU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Classification with Sentence Embeddings & Logistic Regression\n","\n","In this notebook, we’ll walk step-by-step through building a movie-review classifier on the Rotten Tomatoes dataset using pretrained sentence embeddings and scikit-learn’s Logistic Regression. We’ll cover:\n","\n","1. **Environment Setup** – install and import libraries.  \n","2. **Load & Inspect Data** – load the Rotten Tomatoes reviews and take a first look.  \n","3. **Generate Embeddings** – encode each review into a fixed-size vector via sentence-transformers.  \n","4. **Train/Test Split** – partition our data for training and evaluation.  \n","5. **Train Classifier** – fit a Logistic Regression model on embedding features.  \n","6. **Evaluate Performance** – compute accuracy, confusion matrix, and classification report.  \n","7. **Predicting New Texts** – create new reviews and predict their \"freshness/rottenness\""],"metadata":{"id":"jAyA70ym3q5w"}},{"cell_type":"markdown","source":["## **Step 1: Environment Setup**\n","\n","First, install the required libraries (`sentence-transformers` for embeddings, `datasets` for loading our data) and import everything we need."],"metadata":{"id":"_nNnDm8h3u12"}},{"cell_type":"code","source":["# Install sentence-transformers for easy embeddings\n","\n","# Install the Hugging Face datasets library to load Rotten Tomatoes reviews\n"],"metadata":{"id":"oDSyICy_Olz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 2: Load & Inspect Data**\n","We’ll load the [Cornell Movie Review “Rotten Tomatoes” dataset](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes) (just the training split for sample size purposes) and convert it to a pandas DataFrame for easy inspection.\n","- `label = 1` means \"fresh\"\n","- `label = 0` means \"rotten\""],"metadata":{"id":"50Js8z6h322y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6A-_5xvyZt3p"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Load the Rotten Tomatoes dataset\n"]},{"cell_type":"code","source":["# Check class distribution to ensure balance\n"],"metadata":{"id":"HZNJJf9S4HKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 3: Generate Sentence Embeddings**\n","We use the [`SentenceTransformer` wrapper](https://huggingface.co/sentence-transformers), which exposes a simple `.encode()` method. It returns an (n_samples × embedding_dim) NumPy array."],"metadata":{"id":"Jjpexoxf4T2e"}},{"cell_type":"code","source":["# sentence-transformers for pretrained encoder\n","\n","\n","# Instantiate the pretrained embedding model\n","\n","\n","# Encode all review texts to embeddings\n","# convert_to_numpy=True returns a NumPy array\n","# show_progress_bar displays encoding progress\n","\n","\n","# Inspect shape: (n_reviews, embedding_dimension)\n"],"metadata":{"id":"vHAibjI1OtXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 4: Split into Training & Testing Sets**\n","We’ll hold out 20% of our embeddings/labels for testing, using a fixed random_state for reproducibility."],"metadata":{"id":"8sgBdoph4i5C"}},{"cell_type":"code","source":["\n","# Define features (embeddings) and target (labels)\n","\n","\n","# Split embeddings and labels into train/test\n","\n","\n","# Confirm sizes\n"],"metadata":{"id":"mQiricbDP__8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 5: Train Logistic Regression**\n","Fit a logistic regression classifier on our training set. We use the default L2 penalty and solver, and let `scikit-learn` choose sensible defaults."],"metadata":{"id":"p8YscAB24oKi"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","# Initialize the logistic regression model\n","clf = LogisticRegression()\n","\n","# Train on embedding features\n","clf.fit(X_train, y_train)"],"metadata":{"id":"HEneSS6jQwlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 6: Evaluate Performance**\n","We’ll predict on the test set, compute overall accuracy, plot the confusion matrix, and print a detailed classification report (precision, recall, F1-score)."],"metadata":{"id":"Z3x3Dd3n5Irz"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Calculate Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","# Generate and plot Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(6,4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()\n","\n","# Print Classification Report\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"r3EigvoPSG1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 7: Predict New Texts**\n","Now that we have a trained model, let’s see how it does on user-supplied reviews. We’ll:\n","\n","1. Define a few custom review strings.  \n","2. Encode them into embeddings.  \n","3. Predict labels and class-probabilities with our logistic regressor.  \n","4. Print out each review with its predicted sentiment and confidence."],"metadata":{"id":"ZI6jGAhl5H9h"}},{"cell_type":"code","source":["# 7.1 Define some new reviews to classify\n","new_reviews = [\n","    \"I absolutely loved this movie, the performances were stellar!\",\n","    \"This was a terrible film; I wasted two hours of my life.\",\n","    \"The story was okay, but the pacing felt off in the second half.\",\n","    \"An underrated gem—beautiful cinematography and great score.\"\n","]\n","\n","# 7.2 Generate embeddings for these new texts\n","# (using the same embedder we initialized earlier)\n","new_embs = embedder.encode(\n","    new_reviews,\n","    convert_to_numpy=True,\n","    show_progress_bar=False\n",")\n","\n","# 7.3 Predict labels and probabilities\n","new_preds = clf.predict(new_embs)\n","new_probs = clf.predict_proba(new_embs)\n","\n","# 7.4 Display results\n","for text, pred, probs in zip(new_reviews, new_preds, new_probs):\n","    # Map numeric label back to sentiment\n","    label_str = \"Positive\" if pred == 1 else \"Negative\"\n","    confidence = probs[pred]\n","    print(f\"Review: {text!r}\")\n","    print(f\" → Predicted: {label_str} (confidence = {confidence:.3f})\\n\")"],"metadata":{"id":"pOlGAtPF5ZNG"},"execution_count":null,"outputs":[]}]}