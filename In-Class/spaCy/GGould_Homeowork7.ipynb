{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5dckud01fIk"
      },
      "source": [
        "Ginger Gould\n",
        "3/30/2025\n",
        "Elements of Computing II\n",
        "\n",
        "This program was made to explore the tokenization process through python libraries such as spaCy.\n",
        "The text that will be tokenized is : \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbmarU0LTt-J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "does\n",
            "n't\n",
            "jump\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n",
            ".\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "is\n",
            "fascinating\n",
            "!\n"
          ]
        }
      ],
      "source": [
        "## Task 1 - Tokenization\n",
        "# Import Libraries, i.e spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define text to be processed\n",
        "text = \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions and Answer:\n",
        "How does spaCY process the various tokens : SpaCy is breaking down the sentence into words, punctuation, and parts of words. In doing so it is deconstructing the sentence into its most important components.\n",
        "\n",
        "How does spaCY handle punction marks like periods and commas : The punctuation acts as its own token, its own unit of importance. \n",
        "\n",
        "What happens when the text includes contracts (e.g. \"don't\") : The contraction is split into two tokens since the tokenization process is trying to derive parts of a sentence with \"syntatic purpose\" and a contraction has two \"syntatic\" purposes. In this case, the contraction is \"doesn't\" which serves the purpose of the word \"does\" and the word \"not\" or as spaCy tokenizes as, \"n't.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DET DT\n",
            "quick ADJ JJ\n",
            "brown ADJ JJ\n",
            "fox NOUN NN\n",
            "does AUX VBZ\n",
            "n't PART RB\n",
            "jump VERB VB\n",
            "over ADP IN\n",
            "the DET DT\n",
            "lazy ADJ JJ\n",
            "dog NOUN NN\n",
            ". PUNCT .\n",
            "Natural PROPN NNP\n",
            "Language PROPN NNP\n",
            "Processing NOUN NN\n",
            "is AUX VBZ\n",
            "fascinating ADJ JJ\n",
            "! PUNCT .\n"
          ]
        }
      ],
      "source": [
        "## Task 2 - Part-of-Speech Tagging\n",
        "# Extend script to include Part-of-Speech tagging\n",
        "# Import Libraries, i.e spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define text to be processed\n",
        "text = \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
        "\n",
        "# Process the text using spaCy and print tokens with their POS tags\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_,  token.tag_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions and Answers:\n",
        "Identify the POS tags for \"quick,\" \"jumps\", and \"is\" : Quick - defined as an ADJ (assume to be adjective) and tagged as JJ, jumps - defined as VERB and tagged as VB, is - defined as AUX and tagged as VBZ\n",
        "\n",
        "Why might POS tagging be useful for tasks like grammar checking or machine translation : Grammar checking clarifies contextualization, for example, if a word can be interpreted as a verb or noun such as \"dance\" it is important for the model to clarify its actual meaning in the specific situation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama PERSON\n",
            "44th ORDINAL\n",
            "the United States GPE\n",
            "Hawaii GPE\n"
          ]
        }
      ],
      "source": [
        "## Task 3 - Named Entity Recognition (NER)\n",
        "# Extend script to include Named Entity Recognition (NER)\n",
        "# Import Libraries, i.e spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define text to be processed - different than before\n",
        "text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
        "\n",
        "# Process the text using spaCy and print named entities\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions and Answers:\n",
        "Which entities are recognized by spaCy : In this scenerio spaCy recognizes, \"Barack Obama,\" \"44th,\" \"The United States,\" and \"Hawaii\" as entities. \n",
        "\n",
        "What entity types are assigned to \"Barack Obama\" and \"Hawaii\" : The entitiy, \"Barack Obama\" is assigned the PERSON entity type whereas \"Hawaii\" is assigned the GPE entity type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DET DT\n",
            "complex ADJ JJ\n",
            "houses NOUN NNS\n",
            "married VERB VBN\n",
            "and CCONJ CC\n",
            "single ADJ JJ\n",
            "soldiers NOUN NNS\n",
            "and CCONJ CC\n",
            "their PRON PRP$\n",
            "families NOUN NNS\n",
            ". PUNCT .\n",
            "\n",
            "\n",
            "This DET DT\n",
            "sentnce NOUN NN\n",
            "has VERB VBZ\n",
            "a DET DT\n",
            "faw NOUN NN\n",
            "tipos NOUN NN\n",
            ". PUNCT .\n"
          ]
        }
      ],
      "source": [
        "## Take 4 - Experimentation\n",
        "# Import Libraries, i.e spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define text to be processed\n",
        "text1 = \"The complex houses married and single soldiers and their families.\" # Why this text? It contains a correct grammatical structure but is semantically ambiguous as the actual noun is not clear.\n",
        "\n",
        "text2 = \"This sentnce has a faw tipos.\" # Why this text? It contains a few typos and grammatical errors, which can be useful for testing the robustness of the NLP model.\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc1 = nlp(text1)\n",
        "for token in doc1:\n",
        "    print(token.text, token.pos_,  token.tag_)\n",
        "    \n",
        "\n",
        "print(\"\\n\")  # Just to separate the outputs\n",
        "\n",
        "doc2 = nlp(text2)\n",
        "for token in doc2:\n",
        "    print(token.text,  token.pos_,  token.tag_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clare ORG\n",
            "Demark GPE\n"
          ]
        }
      ],
      "source": [
        "# Extend script to include Named Entity Recognition (NER)\n",
        "# Import Libraries, i.e spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define text to be processed - different than before\n",
        "text1 = \"Clare has red hair and lives in Demark.\" # Has a named entity \"Demark\" which is a country.\n",
        "\n",
        "\n",
        "# Process the text using spaCy and print named entities\n",
        "\n",
        "\n",
        "doc1 = nlp(text1)\n",
        "for ent in doc1.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questions and Answers:\n",
        "How does spaCy handle your modifications : In the typo experimentation the model was able to still define the word type of the words that had errors. However, in the complex sentence it defined the noun in the sentence incorrectly, the actual noun is \"complex\" but modeled defined the word \"houses\" as the noun.\n",
        "\n",
        "Did any entities or tags change? Why did this happen? : It was still able to recognize the tag of a misspelled country, but when the name is misspelled from \"Claire\" to \"Clare\" the entity mistakes the noun designation and provides the tag \"ORG\" rather than \"PERSON\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
