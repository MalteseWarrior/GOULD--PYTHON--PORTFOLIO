{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8tusd7NLPtY"
      },
      "outputs": [],
      "source": [
        "## Import Libraries\n",
        "import streamlit as st\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from spacy.pipeline import EntityRuler\n",
        "\n",
        "\n",
        "## ---------- Load Data From GoofyScraper.py ---------- ##\n",
        "SpongeData = pd.read_csv('spongebob_transcripts.csv')\n",
        "\n",
        "# Clear data of NaN values and empty strings\n",
        "SpongeData.dropna(inplace=True)\n",
        "\n",
        "# Remove rows where any critical field is an empty string\n",
        "SpongeData = SpongeData[\n",
        "    (SpongeData.iloc[:, 0].str.strip() != \"\") &  # Episode\n",
        "    (SpongeData.iloc[:, 1].str.strip() != \"\") &  # Character\n",
        "    (SpongeData.iloc[:, 2].str.strip() != \"\")    # Dialogue\n",
        "]\n",
        "\n",
        "SpongeData = SpongeData.astype(str)\n",
        "\n",
        "## ---------- Load the spaCy model for named entity recognition ---------- ##\n",
        "## ---------- Create Patterns for NER ---------- ##\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def location_token_patterns(location_list):\n",
        "    patterns = []\n",
        "    for loc in location_list:\n",
        "        tokens = [{\"LOWER\": word.lower()} for word in loc.strip().split()]\n",
        "        # Add flexible version with optional \"the\"\n",
        "        if tokens[0][\"LOWER\"] != \"the\":\n",
        "            tokens_with_the = [{\"LOWER\": \"the\"}] + tokens\n",
        "            patterns.append({\"label\": \"LOCATION\", \"pattern\": tokens_with_the})\n",
        "        patterns.append({\"label\": \"LOCATION\", \"pattern\": tokens})\n",
        "    return patterns\n",
        "\n",
        " # The dataframe is structured as follows:\n",
        "    # Column 1: Episode Name\n",
        "    # Column 2: Character Name\n",
        "    # Column 3: Character Dialogue\n",
        "    # Because of this the patterns will be based on this structure and through a for loop to capture every character and their dialogue\n",
        "patterns  = []\n",
        "\n",
        "    # Episode\n",
        "for ep in SpongeData[\"ep\"].unique():\n",
        "    patterns.append({\"label\": \"EPISODE\", \"pattern\": ep})\n",
        "\n",
        "# Character - Dataframe lists some actions in character section, usually above 2 words\n",
        "# So event names will be filtered out and put into their own category\n",
        "    \n",
        "for char in SpongeData[\"char\"].unique():\n",
        "    if len(char.split()) > 2:\n",
        "        patterns.append({\"label\": \"EVENT\", \"pattern\": char})\n",
        "    else:\n",
        "        patterns.append({\"label\": \"CHARACTER\", \"pattern\": char})\n",
        "\n",
        "\n",
        "location_names = [\n",
        "    \"Krusty Krab\", \"The Krusty Krab\", \"Krusty Krab Pizza\", \"Chum Bucket\", \"Rock Bottom\",\n",
        "    \"Goo Lagoon\", \"Jellyfish Fields\", \"Sandy's Treedome\", \"Mrs. Puff's Boating School\",\n",
        "    \"Shell City\", \"Glove World\", \"The Salty Spitoon\", \"Bikini Bottom\", \"Karate Island\",\n",
        "    \"The Flying Dutchman's Ship\", \"Bikini Bottom Hospital\", \"Bikini Bottom Library\"]\n",
        "\n",
        "    \n",
        "patterns.extend(location_token_patterns(location_names))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "## ---------- Add Entity Ruler ----------- ##\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "ruler.add_patterns(patterns)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
